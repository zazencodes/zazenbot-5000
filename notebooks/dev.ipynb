{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview import rag\n",
    "import vertexai\n",
    "import os\n",
    "from vertexai.preview.rag.utils.resources import ChunkingConfig, TransformationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"../.credentials/zazencodes-45dfe5f887df.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=\"zazencodes\"\n",
    "LOCATION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created corpus: RagCorpus(name='projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976', display_name='zazenbot-5000-video-transcripts', description='Transcripts and other metadata for ZazenCodes YouTube videos', embedding_model_config=EmbeddingModelConfig(publisher_model='projects/890511813715/locations/us-central1/publishers/google/models/text-embedding-005', endpoint=None, model=None, model_version_id=None), vector_db=RagManagedDb(), vertex_ai_search_config=None, backend_config=RagVectorDbConfig(vector_db=RagManagedDb(), rag_embedding_model_config=None))\n"
     ]
    }
   ],
   "source": [
    "corpus = rag.create_corpus(\n",
    "    display_name=\"zazenbot-5000-video-transcripts\",\n",
    "    description=\"Transcripts and other metadata for ZazenCodes YouTube videos\",\n",
    ")\n",
    "print(\"Created corpus:\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RagCorpus(name='projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976', display_name='zazenbot-5000-video-transcripts', description='Transcripts and other metadata for ZazenCodes YouTube videos', embedding_model_config=EmbeddingModelConfig(publisher_model='projects/890511813715/locations/us-central1/publishers/google/models/text-embedding-005', endpoint=None, model=None, model_version_id=None), vector_db=RagManagedDb(), vertex_ai_search_config=None, backend_config=RagVectorDbConfig(vector_db=RagManagedDb(), rag_embedding_model_config=RagEmbeddingModelConfig(vertex_prediction_endpoint=VertexPredictionEndpoint(endpoint=None, publisher_model='projects/890511813715/locations/us-central1/publishers/google/models/text-embedding-005', model=None, model_version_id=None))))\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976\"\n",
    "corpus = rag.get_corpus(corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete it run this:\n",
    "# rag.delete_corpus(name=corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "rag.import_files(\n",
      "    corpus_name: str,\n",
      "    paths: Optional[Sequence[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    source: Union[vertexai.preview.rag.utils.resources.SlackChannelsSource, vertexai.preview.rag.utils.resources.JiraSource, vertexai.preview.rag.utils.resources.SharePointSources, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    chunk_size: int = \u001b[32m1024\u001b[39m,\n",
      "    chunk_overlap: int = \u001b[32m200\u001b[39m,\n",
      "    transformation_config: Optional[vertexai.preview.rag.utils.resources.TransformationConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    timeout: int = \u001b[32m600\u001b[39m,\n",
      "    max_embedding_requests_per_min: int = \u001b[32m1000\u001b[39m,\n",
      "    use_advanced_pdf_parsing: Optional[bool] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    partial_failures_sink: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    layout_parser: Optional[vertexai.preview.rag.utils.resources.LayoutParserConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    llm_parser: Optional[vertexai.preview.rag.utils.resources.LlmParserConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> google.cloud.aiplatform_v1beta1.types.vertex_rag_data_service.ImportRagFilesResponse\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Import files to an existing RagCorpus, wait until completion.\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```\n",
      "import vertexai\n",
      "from vertexai.preview import rag\n",
      "from google.protobuf import timestamp_pb2\n",
      "\n",
      "vertexai.init(project=\"my-project\")\n",
      "# Google Drive example\n",
      "paths = [\n",
      "    \"https://drive.google.com/file/d/123\",\n",
      "    \"https://drive.google.com/drive/folders/456\"\n",
      "]\n",
      "# Google Cloud Storage example\n",
      "paths = [\"gs://my_bucket/my_files_dir\", ...]\n",
      "\n",
      "transformation_config = TransformationConfig(\n",
      "    chunking_config=ChunkingConfig(\n",
      "        chunk_size=1024,\n",
      "        chunk_overlap=200,\n",
      "    ),\n",
      ")\n",
      "\n",
      "response = rag.import_files(\n",
      "    corpus_name=\"projects/my-project/locations/us-central1/ragCorpora/my-corpus-1\",\n",
      "    paths=paths,\n",
      "    transformation_config=transformation_config,\n",
      ")\n",
      "\n",
      "# Slack example\n",
      "start_time = timestamp_pb2.Timestamp()\n",
      "start_time.FromJsonString('2020-12-31T21:33:44Z')\n",
      "end_time = timestamp_pb2.Timestamp()\n",
      "end_time.GetCurrentTime()\n",
      "source = rag.SlackChannelsSource(\n",
      "    channels = [\n",
      "        SlackChannel(\"channel1\", \"api_key1\"),\n",
      "        SlackChannel(\"channel2\", \"api_key2\", start_time, end_time)\n",
      "    ],\n",
      ")\n",
      "# Jira Example\n",
      "jira_query = rag.JiraQuery(\n",
      "    email=\"xxx@yyy.com\",\n",
      "    jira_projects=[\"project1\", \"project2\"],\n",
      "    custom_queries=[\"query1\", \"query2\"],\n",
      "    api_key=\"api_key\",\n",
      "    server_uri=\"server.atlassian.net\"\n",
      ")\n",
      "source = rag.JiraSource(\n",
      "    queries=[jira_query],\n",
      ")\n",
      "\n",
      "response = rag.import_files(\n",
      "    corpus_name=\"projects/my-project/locations/us-central1/ragCorpora/my-corpus-1\",\n",
      "    source=source,\n",
      "    transformation_config=transformation_config,\n",
      ")\n",
      "\n",
      "# SharePoint Example.\n",
      "sharepoint_query = rag.SharePointSource(\n",
      "    sharepoint_folder_path=\"https://my-sharepoint-site.com/my-folder\",\n",
      "    sharepoint_site_name=\"my-sharepoint-site.com\",\n",
      "    client_id=\"my-client-id\",\n",
      "    client_secret=\"my-client-secret\",\n",
      "    tenant_id=\"my-tenant-id\",\n",
      "    drive_id=\"my-drive-id\",\n",
      ")\n",
      "source = rag.SharePointSources(\n",
      "    share_point_sources=[sharepoint_query],\n",
      ")\n",
      "\n",
      "# Return the number of imported RagFiles after completion.\n",
      "print(response.imported_rag_files_count)\n",
      "\n",
      "```\n",
      "Args:\n",
      "    corpus_name: The name of the RagCorpus resource into which to import files.\n",
      "        Format: ``projects/{project}/locations/{location}/ragCorpora/{rag_corpus}``\n",
      "        or ``{rag_corpus}``.\n",
      "    paths: A list of uris. Eligible uris will be Google Cloud Storage\n",
      "        directory (\"gs://my-bucket/my_dir\") or a Google Drive url for file\n",
      "        (https://drive.google.com/file/... or folder\n",
      "        \"https://drive.google.com/corp/drive/folders/...\").\n",
      "    source: The source of the Slack or Jira import.\n",
      "        Must be either a SlackChannelsSource or JiraSource.\n",
      "    chunk_size: The size of the chunks. This field is deprecated. Please use\n",
      "        transformation_config instead.\n",
      "    chunk_overlap: The overlap between chunks. This field is deprecated. Please use\n",
      "        transformation_config instead.\n",
      "    transformation_config: The config for transforming the imported\n",
      "        RagFiles.\n",
      "    max_embedding_requests_per_min:\n",
      "        Optional. The max number of queries per\n",
      "        minute that this job is allowed to make to the\n",
      "        embedding model specified on the corpus. This\n",
      "        value is specific to this job and not shared\n",
      "        across other import jobs. Consult the Quotas\n",
      "        page on the project to set an appropriate value\n",
      "        here. If unspecified, a default value of 1,000\n",
      "        QPM would be used.\n",
      "    timeout: Default is 600 seconds.\n",
      "    use_advanced_pdf_parsing: Whether to use advanced PDF\n",
      "        parsing on uploaded files. This field is deprecated.\n",
      "    partial_failures_sink: Either a GCS path to store partial failures or a\n",
      "        BigQuery table to store partial failures. The format is\n",
      "        \"gs://my-bucket/my/object.ndjson\" for GCS or\n",
      "        \"bq://my-project.my-dataset.my-table\" for BigQuery. An existing GCS\n",
      "        object cannot be used. However, the BigQuery table may or may not\n",
      "        exist - if it does not exist, it will be created. If it does exist,\n",
      "        the schema will be checked and the partial failures will be appended\n",
      "        to the table.\n",
      "    layout_parser: Configuration for the Document AI Layout Parser Processor\n",
      "        to use for document parsing. Optional.\n",
      "        If not None, the other parser configs must be None.\n",
      "    llm_parser: Configuration for the LLM Parser to use for document parsing.\n",
      "        Optional.\n",
      "        If not None, the other parser configs must be None.\n",
      "Returns:\n",
      "    ImportRagFilesResponse.\n",
      "\u001b[31mFile:\u001b[39m      ~/pro/zazenbot-5000/venv/lib/python3.12/site-packages/vertexai/preview/rag/rag_data.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "rag.import_files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListRagFilesPager<>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.list_files(corpus_name=corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_config = TransformationConfig(\n",
    "    chunking_config=ChunkingConfig(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=100,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = rag.import_files(\n",
    "    corpus_name=corpus_name,\n",
    "    paths=[f\"gs://zazenbot-5000/yt-rag/transcript-markers\"],\n",
    "    transformation_config=transformation_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025_01_08_finally_figured_out_what_to_do.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428653081941886\n",
      "2025_01_21_how_i_use_ai_for_my_data_engineering_work.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428653481984324\n",
      "2025_01_15_the_awesome_power_of_an_llm_in_your_terminal.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428654762461317\n",
      "2025_01_24_12_neovim_ai_data_engineering_use_cases.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428656000817133\n",
      "2025_02_05_100_ai_job_postings_later_heres_whats_actually_in_demand.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428745518407334\n",
      "2025_02_19_5_level_gear_guide_for_modern_tech_workers.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428745844700685\n",
      "2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428746316087794\n",
      "2025_02_26_ai_startup_tier_list_whats_worth_building_in_2025.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428747132421048\n",
      "2025_01_29_data_jobs_in_2025.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428747010734176\n",
      "2025_03_05_i_built_an_ai_physics_agent_that_drafts_research_papers.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428748694723778\n",
      "2025_03_12_learn_how_to_build_tool_calling_agents_with_langgraph.txt\n",
      "projects/890511813715/locations/us-central1/ragCorpora/1152921504606846976/ragFiles/5383428753417460575\n"
     ]
    }
   ],
   "source": [
    "files = rag.list_files(corpus_name=corpus_name)\n",
    "for file in files:\n",
    "    print(file.display_name)\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "rag.retrieval_query(\n",
      "    text: str,\n",
      "    rag_resources: Optional[List[vertexai.preview.rag.utils.resources.RagResource]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    rag_corpora: Optional[List[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    similarity_top_k: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    vector_distance_threshold: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    vector_search_alpha: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    rag_retrieval_config: Optional[vertexai.preview.rag.utils.resources.RagRetrievalConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> google.cloud.aiplatform_v1beta1.types.vertex_rag_service.RetrieveContextsResponse\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Retrieve top k relevant docs/chunks.\n",
      "\n",
      "Example usage:\n",
      "```\n",
      "import vertexai\n",
      "\n",
      "vertexai.init(project=\"my-project\")\n",
      "\n",
      "# Using deprecated parameters\n",
      "results = vertexai.preview.rag.retrieval_query(\n",
      "    text=\"Why is the sky blue?\",\n",
      "    rag_resources=[vertexai.preview.rag.RagResource(\n",
      "        rag_corpus=\"projects/my-project/locations/us-central1/ragCorpora/rag-corpus-1\",\n",
      "        rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
      "    )],\n",
      "    similarity_top_k=2,\n",
      "    vector_distance_threshold=0.5,\n",
      "    vector_search_alpha=0.5,\n",
      ")\n",
      "\n",
      "# Using RagRetrievalConfig. Equivalent to the above example.\n",
      "config = vertexai.preview.rag.RagRetrievalConfig(\n",
      "    top_k=2,\n",
      "    filter=vertexai.preview.rag.Filter(\n",
      "        vector_distance_threshold=0.5\n",
      "    ),\n",
      "    hybrid_search=vertexai.preview.rag.rag_retrieval_config.hybrid_search(\n",
      "        alpha=0.5\n",
      "    ),\n",
      "    ranking=vertex.preview.rag.Ranking(\n",
      "        llm_ranker=vertexai.preview.rag.LlmRanker(\n",
      "            model_name=\"gemini-1.5-flash-002\"\n",
      "        )\n",
      "    )\n",
      ")\n",
      "\n",
      "results = vertexai.preview.rag.retrieval_query(\n",
      "    text=\"Why is the sky blue?\",\n",
      "    rag_resources=[vertexai.preview.rag.RagResource(\n",
      "        rag_corpus=\"projects/my-project/locations/us-central1/ragCorpora/rag-corpus-1\",\n",
      "        rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
      "    )],\n",
      "    rag_retrieval_config=config,\n",
      ")\n",
      "```\n",
      "\n",
      "Args:\n",
      "    text: The query in text format to get relevant contexts.\n",
      "    rag_resources: A list of RagResource. It can be used to specify corpus\n",
      "        only or ragfiles. Currently only support one corpus or multiple files\n",
      "        from one corpus. In the future we may open up multiple corpora support.\n",
      "    rag_corpora: If rag_resources is not specified, use rag_corpora as a list\n",
      "        of rag corpora names. Deprecated. Use rag_resources instead.\n",
      "    similarity_top_k: The number of contexts to retrieve. Deprecated. Use\n",
      "        rag_retrieval_config.top_k instead.\n",
      "    vector_distance_threshold: Optional. Only return contexts with vector\n",
      "        distance smaller than the threshold. Deprecated. Use\n",
      "        rag_retrieval_config.filter.vector_distance_threshold instead.\n",
      "    vector_search_alpha: Optional. Controls the weight between dense and\n",
      "        sparse vector search results. The range is [0, 1], where 0 means\n",
      "        sparse vector search only and 1 means dense vector search only.\n",
      "        The default value is 0.5. Deprecated. Use\n",
      "        rag_retrieval_config.hybrid_search.alpha instead.\n",
      "    rag_retrieval_config: Optional. The config containing the retrieval\n",
      "        parameters, including top_k, vector_distance_threshold,\n",
      "        and alpha.\n",
      "\n",
      "Returns:\n",
      "    RetrieveContextsResonse.\n",
      "\u001b[31mFile:\u001b[39m      ~/pro/zazenbot-5000/venv/lib/python3.12/site-packages/vertexai/preview/rag/rag_retrieval.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "rag.retrieval_query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rag.retrieval_query(\n",
    "    text=\"How do you use ollama with the llm cli?\",\n",
    "    rag_resources=[rag.RagResource(\n",
    "        rag_corpus=corpus_name,\n",
    "        # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
    "    )],\n",
    "    similarity_top_k=3,  # Optional\n",
    "    # vector_distance_threshold=0.5,  # Optional\n",
    "    # rag_retrieval_config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contexts {\n",
       "  contexts {\n",
       "    source_uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "    text: \"[00:18:55] So here it\\'s printed all this stuff. So let me go ahead and yank this, and if I just undo this, so here\\'s what we, here\\'s what we got. And it\\'s doing the printing now, which is great, so I\\'ll get rid of this, and let\\'s see if, let\\'s see if we can run this now. Oh, it\\'s, oh, here we go. Here we go. There we go.\\n\\n[00:19:13] Nice. Nice. Alright. Let\\'s do nature. That should work. Yep. Here\\'s my nature. This is pretty fun. Okay, what about, uh, random? Yo, it did it. Yes. Yes. Continue. Yes. Help. Okay. Um, I wonder how favorites works. Hmm. Anyways, um, great. Okay. So we\\'ve tested, uh, with Avante. using Olama. I\\'ll just get rid of it because I don\\'t actually want to use Olama for this use case as I\\'ve described.\\n\\n[00:19:47] I like using, um, Claude. You could also use OpenAI. I used to use GPT 4 Mini, but I found Claude better and still cost effective. So that\\'s what I\\'m doing for that. So the next thing I want to show you is how to install a command line tool called LLM with Olama. How to integrate that together. Because as we\\'ve seen, we can do like Olama run our model.\\n\\n[00:20:07] And then we can like talk to it and stuff, but I really like this LLM tool. I made a whole video on this LLM command line tool and how to use it. Um, and you can check that video out for some use cases and reasons why I like it, but you can basically run one off prompts with this LLM tool. Um, and that\\'s not something you can do with Olama.\"\n",
       "    distance: 0.25434440173041739\n",
       "    source_display_name: \"2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "    score: 0.25434440173041739\n",
       "  }\n",
       "  contexts {\n",
       "    source_uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "    text: \"[00:20:07] And then we can like talk to it and stuff, but I really like this LLM tool. I made a whole video on this LLM command line tool and how to use it. Um, and you can check that video out for some use cases and reasons why I like it, but you can basically run one off prompts with this LLM tool. Um, and that\\'s not something you can do with Olama.\\n\\n[00:20:25] You have to like go ahead and run these models and you have this like chat window. So you\\'d have to kind of keep, um, you\\'ll see what I\\'m talking about. So what we\\'re going to do now is install this tool like from scratch and then install Olama extension for this tool. So here\\'s the tool installation instructions.\\n\\n[00:20:40] You can do it with pip, but I prefer to use pip X. I recently discovered this tool, um, pip X, and it\\'s really helpful for, uh, I\\'ll show you. So if I go into my second brain notes, I was just taking notes on this, um, over, I have a whole video on like this as well, like the second brain thing that I do, but, uh, yeah, super useful.\\n\\n[00:21:01] So what is PipX? It\\'s a tool for running Python applications in isolated environments, not running, installing and running them. So it installs each application to its own isolated virtual environment, which is really incredible because then you don\\'t pollute your like global Python space. Um, I should make an entire video on this because it\\'s, it\\'s really very cool, but here I even showed the example, um, of using this with, uh, LLM.\\n\\n[00:21:27] Okay, so let\\'s go ahead and install LLM. So I\\'ll say pipx install llm, and it\\'s gone ahead and done that using Python 3. 13. So now I can say something like which llm, and I can see it\\'s here in this, in my home user directory local bin llm.\"\n",
       "    distance: 0.25913937877098303\n",
       "    source_display_name: \"2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "    score: 0.25913937877098303\n",
       "  }\n",
       "  contexts {\n",
       "    source_uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "    text: \"[00:01:20] But let\\'s say I did that. I could also go, I could also go up to my toolbar and I see this little Lama guy. And I can go ahead and quit that. Okay, so now it\\'s, it\\'s been quit and I can do something called olama serve. And this is going to run my model. So, I\\'m doing that now in my command line instead of the toolbar.\\n\\n[00:01:38] But it works the same way. And it\\'s going to be running on this, uh, on this port. So let me Let me show you what that port is. Um, it was one, one. Okay. So here\\'s the port right here and I can type this command LSOF dash I, and I can sort of look at what\\'s going on on that port. So here, I\\'m doing that down here and I\\'m using TMUX, um, to just kind of be able to hop around and run.\\n\\n[00:02:02] Uh, multiple terminal windows on my screen. So here, here is actually Olama running. Um, and it\\'s running on this port 11, 4, 3, 4. So if I shut it down up here, I\\'m going to run my command again. And now notice it\\'s not running anymore. So that\\'s how Olama works. It\\'s just like running on a port on your computer.\\n\\n[00:02:21] Um, but how, how do we like use Olama? Okay. So I want to sort of rearrange these windows a little bit. Bring this one like this. Okay. And then I want to move that over to the left. Now, you can see my little, uh, commands down here, if you\\'re interested how I\\'m doing this sort of stuff. Uh, right. Okay, so Ollama is running at the top right now, and we\\'re just gonna, I kind of have this little guide we\\'re gonna walk through.\"\n",
       "    distance: 0.265243748393719\n",
       "    source_display_name: \"2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "    score: 0.265243748393719\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results are sorted with lowest (best) score firstq\n",
    "# Chat about how to interpret scores from vertex AI: https://chatgpt.com/share/67c9a183-f37c-8004-b6da-555a56f49e16\n",
    "# TLDR: the score represents the cosine distance, defined as 1 minus the cosine similarity: 0 is the best. 2 is the worst.\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.contexts.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "text: \"[00:18:55] So here it\\'s printed all this stuff. So let me go ahead and yank this, and if I just undo this, so here\\'s what we, here\\'s what we got. And it\\'s doing the printing now, which is great, so I\\'ll get rid of this, and let\\'s see if, let\\'s see if we can run this now. Oh, it\\'s, oh, here we go. Here we go. There we go.\\n\\n[00:19:13] Nice. Nice. Alright. Let\\'s do nature. That should work. Yep. Here\\'s my nature. This is pretty fun. Okay, what about, uh, random? Yo, it did it. Yes. Yes. Continue. Yes. Help. Okay. Um, I wonder how favorites works. Hmm. Anyways, um, great. Okay. So we\\'ve tested, uh, with Avante. using Olama. I\\'ll just get rid of it because I don\\'t actually want to use Olama for this use case as I\\'ve described.\\n\\n[00:19:47] I like using, um, Claude. You could also use OpenAI. I used to use GPT 4 Mini, but I found Claude better and still cost effective. So that\\'s what I\\'m doing for that. So the next thing I want to show you is how to install a command line tool called LLM with Olama. How to integrate that together. Because as we\\'ve seen, we can do like Olama run our model.\\n\\n[00:20:07] And then we can like talk to it and stuff, but I really like this LLM tool. I made a whole video on this LLM command line tool and how to use it. Um, and you can check that video out for some use cases and reasons why I like it, but you can basically run one off prompts with this LLM tool. Um, and that\\'s not something you can do with Olama.\"\n",
       "distance: 0.25434440173041739\n",
       "source_display_name: \"2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "score: 0.25434440173041739"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.contexts.contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2543444017304174"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.contexts.contexts[0].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel, Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Ollama with the LLM command line tool, you first need to install the LLM tool. The suggested method for installation is using PipX, which installs the application in an isolated environment. After installing LLM, you can then integrate it with Ollama. The provided sources do not contain the exact steps for the integration, but they do mention that you can run one-off prompts with the LLM tool, which is not something you can do with Ollama alone. You also need to ensure Ollama is running and that the desired model is installed and available.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_retrieval_tool = Tool.from_retrieval(\n",
    "    retrieval=rag.Retrieval(\n",
    "        source=rag.VertexRagStore(\n",
    "            rag_resources=[\n",
    "                rag.RagResource(\n",
    "                    rag_corpus=corpus_name,\n",
    "                    # Optional: supply IDs from `rag.list_files()`.\n",
    "                    # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
    "                )\n",
    "            ],\n",
    "            similarity_top_k=3,  # Optional\n",
    "            # vector_distance_threshold=VECTOR_DISTANCE_THRESHOLD,  # Optional\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "rag_model = GenerativeModel(\n",
    "    # model_name=\"gemini-2.0-flash-lite-001\", # 400 Unable to submit request because Grounding is not supported\n",
    "    # model_name=\"gemini-2.0-flash-lite\", # 400 Unable to submit request because Grounding is not supported\n",
    "    model_name=\"gemini-2.0-flash-001\",\n",
    "    # model_name=\"gemini-1.5-flash-002\",\n",
    "    tools=[rag_retrieval_tool],\n",
    ")\n",
    "response = rag_model.generate_content(\"How do you use ollama with the llm cli?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      text: \"To use Ollama with the LLM command line tool, you first need to install the LLM tool. The suggested method for installation is using PipX, which installs the application in an isolated environment. After installing LLM, you can then integrate it with Ollama. The provided sources do not contain the exact steps for the integration, but they do mention that you can run one-off prompts with the LLM tool, which is not something you can do with Ollama alone. You also need to ensure Ollama is running and that the desired model is installed and available.\\n\"\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  grounding_metadata {\n",
       "    retrieval_queries: \"how to use ollama with the llm cli?\"\n",
       "    grounding_chunks {\n",
       "      retrieved_context {\n",
       "        uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "        title: \"2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "        text: \"[00:18:55] So here it\\'s printed all this stuff. So let me go ahead and yank this, and if I just undo this, so here\\'s what we, here\\'s what we got. And it\\'s doing the printing now, which is great, so I\\'ll get rid of this, and let\\'s see if, let\\'s see if we can run this now. Oh, it\\'s, oh, here we go. Here we go. There we go.\\n\\n[00:19:13] Nice. Nice. Alright. Let\\'s do nature. That should work. Yep. Here\\'s my nature. This is pretty fun. Okay, what about, uh, random? Yo, it did it. Yes. Yes. Continue. Yes. Help. Okay. Um, I wonder how favorites works. Hmm. Anyways, um, great. Okay. So we\\'ve tested, uh, with Avante. using Olama. I\\'ll just get rid of it because I don\\'t actually want to use Olama for this use case as I\\'ve described.\\n\\n[00:19:47] I like using, um, Claude. You could also use OpenAI. I used to use GPT 4 Mini, but I found Claude better and still cost effective. So that\\'s what I\\'m doing for that. So the next thing I want to show you is how to install a command line tool called LLM with Olama. How to integrate that together. Because as we\\'ve seen, we can do like Olama run our model.\\n\\n[00:20:07] And then we can like talk to it and stuff, but I really like this LLM tool. I made a whole video on this LLM command line tool and how to use it. Um, and you can check that video out for some use cases and reasons why I like it, but you can basically run one off prompts with this LLM tool. Um, and that\\'s not something you can do with Olama.\"\n",
       "      }\n",
       "    }\n",
       "    grounding_chunks {\n",
       "      retrieved_context {\n",
       "        uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "        title: \"2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "        text: \"[00:20:07] And then we can like talk to it and stuff, but I really like this LLM tool. I made a whole video on this LLM command line tool and how to use it. Um, and you can check that video out for some use cases and reasons why I like it, but you can basically run one off prompts with this LLM tool. Um, and that\\'s not something you can do with Olama.\\n\\n[00:20:25] You have to like go ahead and run these models and you have this like chat window. So you\\'d have to kind of keep, um, you\\'ll see what I\\'m talking about. So what we\\'re going to do now is install this tool like from scratch and then install Olama extension for this tool. So here\\'s the tool installation instructions.\\n\\n[00:20:40] You can do it with pip, but I prefer to use pip X. I recently discovered this tool, um, pip X, and it\\'s really helpful for, uh, I\\'ll show you. So if I go into my second brain notes, I was just taking notes on this, um, over, I have a whole video on like this as well, like the second brain thing that I do, but, uh, yeah, super useful.\\n\\n[00:21:01] So what is PipX? It\\'s a tool for running Python applications in isolated environments, not running, installing and running them. So it installs each application to its own isolated virtual environment, which is really incredible because then you don\\'t pollute your like global Python space. Um, I should make an entire video on this because it\\'s, it\\'s really very cool, but here I even showed the example, um, of using this with, uh, LLM.\\n\\n[00:21:27] Okay, so let\\'s go ahead and install LLM. So I\\'ll say pipx install llm, and it\\'s gone ahead and done that using Python 3. 13. So now I can say something like which llm, and I can see it\\'s here in this, in my home user directory local bin llm.\"\n",
       "      }\n",
       "    }\n",
       "    grounding_chunks {\n",
       "      retrieved_context {\n",
       "        uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_03_12_learn_how_to_build_tool_calling_agents_with_langgraph.txt\"\n",
       "        title: \"2025_03_12_learn_how_to_build_tool_calling_agents_with_langgraph.txt\"\n",
       "        text: \"Um, I need to make sure olama\\'s running.\\n\\n[00:18:22] So I\\'m gonna do that and there\\'s a, um, if I just type olama, I can see what I have available and I have a video on the olama CLI that you can, uh, you can check out if you\\'re interested in that. So I\\'m going to serve with Olamas. This is make sure Olamas running on the port that I expect it to be on.\\n\\n[00:18:39] Let\\'s go back to, uh, over here. So this, this model also needs to be installed, um, which it is. So I can check that just back where I\\'m going to run this code. Uh, what was it? Olama list. Okay. So here\\'s the model. And so I have it available. And, um, and yeah, let\\'s give this a shot. So I\\'m going to, uh, I\\'m going to rerun my tool.\\n\\n[00:19:00] Chat Olama is not defined. It should be defined, pretty sure. Did I not save? I didn\\'t save it. Okay. Um, okay, let\\'s go back. So this graph is, is working. That\\'s pretty cool. Um, we\\'ve gone ahead and sent off a human message to Grok. Uh, and you\\'ll notice that this, this, um, grok that I\\'m using, it has tool calling available.\\n\\n[00:19:22] So, okay, so check it out. Now we\\'re making HTTP requests to localhost, and this is where Olama is running on my computer. And we\\'re using this API chat endpoint in order to talk to this model. Uh, that\\'s awesome. Okay. So let\\'s try QuantFinance. Okay, quant is a fascinating field. Yeah, let\\'s do risk management.\\n\\n[00:19:43] Sure. Great choice. Let\\'s say, actually, no. I want, uh, neutrinos. I see, neutrinos. Um, it\\'s no.\"\n",
       "      }\n",
       "    }\n",
       "    grounding_supports {\n",
       "      segment {\n",
       "        start_index: 86\n",
       "        end_index: 197\n",
       "        text: \"The suggested method for installation is using PipX, which installs the application in an isolated environment.\"\n",
       "      }\n",
       "      grounding_chunk_indices: 1\n",
       "      confidence_scores: 0.814005077\n",
       "    }\n",
       "    grounding_supports {\n",
       "      segment {\n",
       "        start_index: 457\n",
       "        end_index: 553\n",
       "        text: \"You also need to ensure Ollama is running and that the desired model is installed and available.\"\n",
       "      }\n",
       "      grounding_chunk_indices: 2\n",
       "      confidence_scores: 0.65144372\n",
       "    }\n",
       "  }\n",
       "  avg_logprobs: -0.18256711151640295\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 12\n",
       "  candidates_token_count: 118\n",
       "  total_token_count: 130\n",
       "  prompt_tokens_details {\n",
       "    modality: TEXT\n",
       "    token_count: 12\n",
       "  }\n",
       "  candidates_tokens_details {\n",
       "    modality: TEXT\n",
       "    token_count: 118\n",
       "  }\n",
       "}\n",
       "model_version: \"gemini-2.0-flash-001\"\n",
       "create_time {\n",
       "  seconds: 1741266846\n",
       "  nanos: 800635000\n",
       "}\n",
       "response_id: \"np_JZ_vuMMfvhMIPj8qO6Q8\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retrieved_context {\n",
       "  uri: \"gs://zazenbot-5000/yt-rag/transcript-markers/2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "  title: \"2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt\"\n",
       "  text: \"[00:18:55] So here it\\'s printed all this stuff. So let me go ahead and yank this, and if I just undo this, so here\\'s what we, here\\'s what we got. And it\\'s doing the printing now, which is great, so I\\'ll get rid of this, and let\\'s see if, let\\'s see if we can run this now. Oh, it\\'s, oh, here we go. Here we go. There we go.\\n\\n[00:19:13] Nice. Nice. Alright. Let\\'s do nature. That should work. Yep. Here\\'s my nature. This is pretty fun. Okay, what about, uh, random? Yo, it did it. Yes. Yes. Continue. Yes. Help. Okay. Um, I wonder how favorites works. Hmm. Anyways, um, great. Okay. So we\\'ve tested, uh, with Avante. using Olama. I\\'ll just get rid of it because I don\\'t actually want to use Olama for this use case as I\\'ve described.\\n\\n[00:19:47] I like using, um, Claude. You could also use OpenAI. I used to use GPT 4 Mini, but I found Claude better and still cost effective. So that\\'s what I\\'m doing for that. So the next thing I want to show you is how to install a command line tool called LLM with Olama. How to integrate that together. Because as we\\'ve seen, we can do like Olama run our model.\\n\\n[00:20:07] And then we can like talk to it and stuff, but I really like this LLM tool. I made a whole video on this LLM command line tool and how to use it. Um, and you can check that video out for some use cases and reasons why I like it, but you can basically run one off prompts with this LLM tool. Um, and that\\'s not something you can do with Olama.\"\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].grounding_metadata.grounding_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025_02_12_how_to_setup_and_run_deepseek_r1_locally_using_ollama.txt'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].grounding_metadata.grounding_chunks[0].retrieved_context.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
